{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "alchemy not available, to install alchemy, run `pip install alchemy-catalyst`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from PIL import ImageFilter, Image\n",
    "import torchvision.models as models\n",
    "import pretrainedmodels as ptmodels\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "#import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init\n",
    "\n",
    "from network.meso import Meso4, MesoInception4\n",
    "from network.models import model_selection\n",
    "\n",
    "#from ff import model_selection\n",
    "from pipeline.metrics import accuracy_b, log_loss\n",
    "from pipeline.model_methods import validate_img, train_img, validate_vid, train_vid\n",
    "from pipeline.data_loaders import load_img_dataset, load_img_val_dataset, strong_aug\n",
    "from pipeline.image_extracting import extract_faces, FastMTCNN\n",
    "\n",
    "from pipeline.metrics import accuracy_sigmoid, accuracy_sigmoid_mean, log_loss_sigmoid, log_loss_b, accuracy_b_mean\n",
    "from pipeline.model_methods import validate_vid_bf, train_vid_bf\n",
    "from pipeline.image_extracting import extract_faces, FastMTCNN, InceptionResnetV1, extract_faces_dlib, MTCNN\n",
    "from pipeline.blazeface import BlazeFace\n",
    "from network.lstm_deepfake import LSTMDF\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from pytorchcv.model_provider import get_model\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.contrib.nn import OneCycleLRWithWarmup\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "PATH = \"model.h5\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 19154)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.read_csv(r'D:\\Machine Learning\\deepfake-detection\\data\\metadata.csv')\n",
    "len(y_train[y_train.label == 1]), len(y_train[y_train.label == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%script false\n",
    "\n",
    "batch_size = 16\n",
    "normalize = torchvision.transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "\n",
    "x_train = load_img_dataset(r'data\\train2', batch_size, resize=224, normalize=normalize)\n",
    "x_val = load_img_val_dataset('data\\img_val', 10, resize=224, normalize=normalize)\n",
    "\n",
    "x_train_vid = r'D:\\Machine Learning\\deepfake-detection\\data\\train_set'\n",
    "x_val_vid = r'data\\val'\n",
    "x_test_vid = r'D:\\Machine Learning\\deepfake-detection\\data\\cross_val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    }
   ],
   "source": [
    "#model = EfficientNet.from_name('efficientnet-b0')\n",
    "model = EfficientNet.from_pretrained('efficientnet-b3', num_classes=1)\n",
    "#model = models.resnext50_32x4d(pretrained=True) \n",
    "#model =  models.resnext101_32x8d(pretrained=True) \n",
    "#model =  MyResNeXt()\n",
    "#model = models.resnet18(pretrained=True)\n",
    "#model._fc = nn.Linear(model._fc.in_features, 1)\n",
    "#model.fc = nn.Linear(model.fc.in_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "found = False\n",
    "for child in model.named_parameters():  \n",
    "    #'''\n",
    "    if child[0] == 'layer4.0.conv2.weight':\n",
    "        found = True\n",
    "    #'''\n",
    "    child[1].requires_grad = found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.) \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.) \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, 0.001, 0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(x_train), epochs=10)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: epoch // 30, lambda epoch: 0.95 ** epoch], last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_conv_stem.weight',\n",
       " '_bn0.weight',\n",
       " '_bn0.bias',\n",
       " '_blocks.0._depthwise_conv.weight',\n",
       " '_blocks.0._bn1.weight',\n",
       " '_blocks.0._bn1.bias',\n",
       " '_blocks.0._se_reduce.weight',\n",
       " '_blocks.0._se_reduce.bias',\n",
       " '_blocks.0._se_expand.weight',\n",
       " '_blocks.0._se_expand.bias',\n",
       " '_blocks.0._project_conv.weight',\n",
       " '_blocks.0._bn2.weight',\n",
       " '_blocks.0._bn2.bias',\n",
       " '_blocks.1._depthwise_conv.weight',\n",
       " '_blocks.1._bn1.weight',\n",
       " '_blocks.1._bn1.bias',\n",
       " '_blocks.1._se_reduce.weight',\n",
       " '_blocks.1._se_reduce.bias',\n",
       " '_blocks.1._se_expand.weight',\n",
       " '_blocks.1._se_expand.bias',\n",
       " '_blocks.1._project_conv.weight',\n",
       " '_blocks.1._bn2.weight',\n",
       " '_blocks.1._bn2.bias',\n",
       " '_blocks.2._expand_conv.weight',\n",
       " '_blocks.2._bn0.weight',\n",
       " '_blocks.2._bn0.bias',\n",
       " '_blocks.2._depthwise_conv.weight',\n",
       " '_blocks.2._bn1.weight',\n",
       " '_blocks.2._bn1.bias',\n",
       " '_blocks.2._se_reduce.weight',\n",
       " '_blocks.2._se_reduce.bias',\n",
       " '_blocks.2._se_expand.weight',\n",
       " '_blocks.2._se_expand.bias',\n",
       " '_blocks.2._project_conv.weight',\n",
       " '_blocks.2._bn2.weight',\n",
       " '_blocks.2._bn2.bias',\n",
       " '_blocks.3._expand_conv.weight',\n",
       " '_blocks.3._bn0.weight',\n",
       " '_blocks.3._bn0.bias',\n",
       " '_blocks.3._depthwise_conv.weight',\n",
       " '_blocks.3._bn1.weight',\n",
       " '_blocks.3._bn1.bias',\n",
       " '_blocks.3._se_reduce.weight',\n",
       " '_blocks.3._se_reduce.bias',\n",
       " '_blocks.3._se_expand.weight',\n",
       " '_blocks.3._se_expand.bias',\n",
       " '_blocks.3._project_conv.weight',\n",
       " '_blocks.3._bn2.weight',\n",
       " '_blocks.3._bn2.bias',\n",
       " '_blocks.4._expand_conv.weight',\n",
       " '_blocks.4._bn0.weight',\n",
       " '_blocks.4._bn0.bias',\n",
       " '_blocks.4._depthwise_conv.weight',\n",
       " '_blocks.4._bn1.weight',\n",
       " '_blocks.4._bn1.bias',\n",
       " '_blocks.4._se_reduce.weight',\n",
       " '_blocks.4._se_reduce.bias',\n",
       " '_blocks.4._se_expand.weight',\n",
       " '_blocks.4._se_expand.bias',\n",
       " '_blocks.4._project_conv.weight',\n",
       " '_blocks.4._bn2.weight',\n",
       " '_blocks.4._bn2.bias',\n",
       " '_blocks.5._expand_conv.weight',\n",
       " '_blocks.5._bn0.weight',\n",
       " '_blocks.5._bn0.bias',\n",
       " '_blocks.5._depthwise_conv.weight',\n",
       " '_blocks.5._bn1.weight',\n",
       " '_blocks.5._bn1.bias',\n",
       " '_blocks.5._se_reduce.weight',\n",
       " '_blocks.5._se_reduce.bias',\n",
       " '_blocks.5._se_expand.weight',\n",
       " '_blocks.5._se_expand.bias',\n",
       " '_blocks.5._project_conv.weight',\n",
       " '_blocks.5._bn2.weight',\n",
       " '_blocks.5._bn2.bias',\n",
       " '_blocks.6._expand_conv.weight',\n",
       " '_blocks.6._bn0.weight',\n",
       " '_blocks.6._bn0.bias',\n",
       " '_blocks.6._depthwise_conv.weight',\n",
       " '_blocks.6._bn1.weight',\n",
       " '_blocks.6._bn1.bias',\n",
       " '_blocks.6._se_reduce.weight',\n",
       " '_blocks.6._se_reduce.bias',\n",
       " '_blocks.6._se_expand.weight',\n",
       " '_blocks.6._se_expand.bias',\n",
       " '_blocks.6._project_conv.weight',\n",
       " '_blocks.6._bn2.weight',\n",
       " '_blocks.6._bn2.bias',\n",
       " '_blocks.7._expand_conv.weight',\n",
       " '_blocks.7._bn0.weight',\n",
       " '_blocks.7._bn0.bias',\n",
       " '_blocks.7._depthwise_conv.weight',\n",
       " '_blocks.7._bn1.weight',\n",
       " '_blocks.7._bn1.bias',\n",
       " '_blocks.7._se_reduce.weight',\n",
       " '_blocks.7._se_reduce.bias',\n",
       " '_blocks.7._se_expand.weight',\n",
       " '_blocks.7._se_expand.bias',\n",
       " '_blocks.7._project_conv.weight',\n",
       " '_blocks.7._bn2.weight',\n",
       " '_blocks.7._bn2.bias',\n",
       " '_blocks.8._expand_conv.weight',\n",
       " '_blocks.8._bn0.weight',\n",
       " '_blocks.8._bn0.bias',\n",
       " '_blocks.8._depthwise_conv.weight',\n",
       " '_blocks.8._bn1.weight',\n",
       " '_blocks.8._bn1.bias',\n",
       " '_blocks.8._se_reduce.weight',\n",
       " '_blocks.8._se_reduce.bias',\n",
       " '_blocks.8._se_expand.weight',\n",
       " '_blocks.8._se_expand.bias',\n",
       " '_blocks.8._project_conv.weight',\n",
       " '_blocks.8._bn2.weight',\n",
       " '_blocks.8._bn2.bias',\n",
       " '_blocks.9._expand_conv.weight',\n",
       " '_blocks.9._bn0.weight',\n",
       " '_blocks.9._bn0.bias',\n",
       " '_blocks.9._depthwise_conv.weight',\n",
       " '_blocks.9._bn1.weight',\n",
       " '_blocks.9._bn1.bias',\n",
       " '_blocks.9._se_reduce.weight',\n",
       " '_blocks.9._se_reduce.bias',\n",
       " '_blocks.9._se_expand.weight',\n",
       " '_blocks.9._se_expand.bias',\n",
       " '_blocks.9._project_conv.weight',\n",
       " '_blocks.9._bn2.weight',\n",
       " '_blocks.9._bn2.bias',\n",
       " '_blocks.10._expand_conv.weight',\n",
       " '_blocks.10._bn0.weight',\n",
       " '_blocks.10._bn0.bias',\n",
       " '_blocks.10._depthwise_conv.weight',\n",
       " '_blocks.10._bn1.weight',\n",
       " '_blocks.10._bn1.bias',\n",
       " '_blocks.10._se_reduce.weight',\n",
       " '_blocks.10._se_reduce.bias',\n",
       " '_blocks.10._se_expand.weight',\n",
       " '_blocks.10._se_expand.bias',\n",
       " '_blocks.10._project_conv.weight',\n",
       " '_blocks.10._bn2.weight',\n",
       " '_blocks.10._bn2.bias',\n",
       " '_blocks.11._expand_conv.weight',\n",
       " '_blocks.11._bn0.weight',\n",
       " '_blocks.11._bn0.bias',\n",
       " '_blocks.11._depthwise_conv.weight',\n",
       " '_blocks.11._bn1.weight',\n",
       " '_blocks.11._bn1.bias',\n",
       " '_blocks.11._se_reduce.weight',\n",
       " '_blocks.11._se_reduce.bias',\n",
       " '_blocks.11._se_expand.weight',\n",
       " '_blocks.11._se_expand.bias',\n",
       " '_blocks.11._project_conv.weight',\n",
       " '_blocks.11._bn2.weight',\n",
       " '_blocks.11._bn2.bias',\n",
       " '_blocks.12._expand_conv.weight',\n",
       " '_blocks.12._bn0.weight',\n",
       " '_blocks.12._bn0.bias',\n",
       " '_blocks.12._depthwise_conv.weight',\n",
       " '_blocks.12._bn1.weight',\n",
       " '_blocks.12._bn1.bias',\n",
       " '_blocks.12._se_reduce.weight',\n",
       " '_blocks.12._se_reduce.bias',\n",
       " '_blocks.12._se_expand.weight',\n",
       " '_blocks.12._se_expand.bias',\n",
       " '_blocks.12._project_conv.weight',\n",
       " '_blocks.12._bn2.weight',\n",
       " '_blocks.12._bn2.bias',\n",
       " '_blocks.13._expand_conv.weight',\n",
       " '_blocks.13._bn0.weight',\n",
       " '_blocks.13._bn0.bias',\n",
       " '_blocks.13._depthwise_conv.weight',\n",
       " '_blocks.13._bn1.weight',\n",
       " '_blocks.13._bn1.bias',\n",
       " '_blocks.13._se_reduce.weight',\n",
       " '_blocks.13._se_reduce.bias',\n",
       " '_blocks.13._se_expand.weight',\n",
       " '_blocks.13._se_expand.bias',\n",
       " '_blocks.13._project_conv.weight',\n",
       " '_blocks.13._bn2.weight',\n",
       " '_blocks.13._bn2.bias',\n",
       " '_blocks.14._expand_conv.weight',\n",
       " '_blocks.14._bn0.weight',\n",
       " '_blocks.14._bn0.bias',\n",
       " '_blocks.14._depthwise_conv.weight',\n",
       " '_blocks.14._bn1.weight',\n",
       " '_blocks.14._bn1.bias',\n",
       " '_blocks.14._se_reduce.weight',\n",
       " '_blocks.14._se_reduce.bias',\n",
       " '_blocks.14._se_expand.weight',\n",
       " '_blocks.14._se_expand.bias',\n",
       " '_blocks.14._project_conv.weight',\n",
       " '_blocks.14._bn2.weight',\n",
       " '_blocks.14._bn2.bias',\n",
       " '_blocks.15._expand_conv.weight',\n",
       " '_blocks.15._bn0.weight',\n",
       " '_blocks.15._bn0.bias',\n",
       " '_blocks.15._depthwise_conv.weight',\n",
       " '_blocks.15._bn1.weight',\n",
       " '_blocks.15._bn1.bias',\n",
       " '_blocks.15._se_reduce.weight',\n",
       " '_blocks.15._se_reduce.bias',\n",
       " '_blocks.15._se_expand.weight',\n",
       " '_blocks.15._se_expand.bias',\n",
       " '_blocks.15._project_conv.weight',\n",
       " '_blocks.15._bn2.weight',\n",
       " '_blocks.15._bn2.bias',\n",
       " '_blocks.16._expand_conv.weight',\n",
       " '_blocks.16._bn0.weight',\n",
       " '_blocks.16._bn0.bias',\n",
       " '_blocks.16._depthwise_conv.weight',\n",
       " '_blocks.16._bn1.weight',\n",
       " '_blocks.16._bn1.bias',\n",
       " '_blocks.16._se_reduce.weight',\n",
       " '_blocks.16._se_reduce.bias',\n",
       " '_blocks.16._se_expand.weight',\n",
       " '_blocks.16._se_expand.bias',\n",
       " '_blocks.16._project_conv.weight',\n",
       " '_blocks.16._bn2.weight',\n",
       " '_blocks.16._bn2.bias',\n",
       " '_blocks.17._expand_conv.weight',\n",
       " '_blocks.17._bn0.weight',\n",
       " '_blocks.17._bn0.bias',\n",
       " '_blocks.17._depthwise_conv.weight',\n",
       " '_blocks.17._bn1.weight',\n",
       " '_blocks.17._bn1.bias',\n",
       " '_blocks.17._se_reduce.weight',\n",
       " '_blocks.17._se_reduce.bias',\n",
       " '_blocks.17._se_expand.weight',\n",
       " '_blocks.17._se_expand.bias',\n",
       " '_blocks.17._project_conv.weight',\n",
       " '_blocks.17._bn2.weight',\n",
       " '_blocks.17._bn2.bias',\n",
       " '_blocks.18._expand_conv.weight',\n",
       " '_blocks.18._bn0.weight',\n",
       " '_blocks.18._bn0.bias',\n",
       " '_blocks.18._depthwise_conv.weight',\n",
       " '_blocks.18._bn1.weight',\n",
       " '_blocks.18._bn1.bias',\n",
       " '_blocks.18._se_reduce.weight',\n",
       " '_blocks.18._se_reduce.bias',\n",
       " '_blocks.18._se_expand.weight',\n",
       " '_blocks.18._se_expand.bias',\n",
       " '_blocks.18._project_conv.weight',\n",
       " '_blocks.18._bn2.weight',\n",
       " '_blocks.18._bn2.bias',\n",
       " '_blocks.19._expand_conv.weight',\n",
       " '_blocks.19._bn0.weight',\n",
       " '_blocks.19._bn0.bias',\n",
       " '_blocks.19._depthwise_conv.weight',\n",
       " '_blocks.19._bn1.weight',\n",
       " '_blocks.19._bn1.bias',\n",
       " '_blocks.19._se_reduce.weight',\n",
       " '_blocks.19._se_reduce.bias',\n",
       " '_blocks.19._se_expand.weight',\n",
       " '_blocks.19._se_expand.bias',\n",
       " '_blocks.19._project_conv.weight',\n",
       " '_blocks.19._bn2.weight',\n",
       " '_blocks.19._bn2.bias',\n",
       " '_blocks.20._expand_conv.weight',\n",
       " '_blocks.20._bn0.weight',\n",
       " '_blocks.20._bn0.bias',\n",
       " '_blocks.20._depthwise_conv.weight',\n",
       " '_blocks.20._bn1.weight',\n",
       " '_blocks.20._bn1.bias',\n",
       " '_blocks.20._se_reduce.weight',\n",
       " '_blocks.20._se_reduce.bias',\n",
       " '_blocks.20._se_expand.weight',\n",
       " '_blocks.20._se_expand.bias',\n",
       " '_blocks.20._project_conv.weight',\n",
       " '_blocks.20._bn2.weight',\n",
       " '_blocks.20._bn2.bias',\n",
       " '_blocks.21._expand_conv.weight',\n",
       " '_blocks.21._bn0.weight',\n",
       " '_blocks.21._bn0.bias',\n",
       " '_blocks.21._depthwise_conv.weight',\n",
       " '_blocks.21._bn1.weight',\n",
       " '_blocks.21._bn1.bias',\n",
       " '_blocks.21._se_reduce.weight',\n",
       " '_blocks.21._se_reduce.bias',\n",
       " '_blocks.21._se_expand.weight',\n",
       " '_blocks.21._se_expand.bias',\n",
       " '_blocks.21._project_conv.weight',\n",
       " '_blocks.21._bn2.weight',\n",
       " '_blocks.21._bn2.bias',\n",
       " '_blocks.22._expand_conv.weight',\n",
       " '_blocks.22._bn0.weight',\n",
       " '_blocks.22._bn0.bias',\n",
       " '_blocks.22._depthwise_conv.weight',\n",
       " '_blocks.22._bn1.weight',\n",
       " '_blocks.22._bn1.bias',\n",
       " '_blocks.22._se_reduce.weight',\n",
       " '_blocks.22._se_reduce.bias',\n",
       " '_blocks.22._se_expand.weight',\n",
       " '_blocks.22._se_expand.bias',\n",
       " '_blocks.22._project_conv.weight',\n",
       " '_blocks.22._bn2.weight',\n",
       " '_blocks.22._bn2.bias',\n",
       " '_conv_head.weight',\n",
       " '_bn1.weight',\n",
       " '_bn1.bias',\n",
       " '_fc.weight',\n",
       " '_fc.bias']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0] for i in model.named_parameters() if i[1].requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4459171883760917, 0.45239126635215615)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.37011126635215613 * 100 / 83, 0.37011126635215613 + 0.08228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519c5f3982454f32aa5672fab1d5396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc128a31b7e844d7be2a7cdcbb8eebec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2770.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40471064b1ee478e8688d06f668f853d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1213.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: metrics  0.7435030113026978 loss  0.5686379752635513\n",
      "Train: metrics  0.7296028880866426 loss  0.5278959004295862\n",
      "Expected LB value: 0.6509179752635513 0.6851059942934353\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a1721d285b4bb090ca4dea0bee0015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2770.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68995515e1246dba9f4fc224d6a16e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1213.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: metrics  0.788218793828892 loss  0.4587039594421039\n",
      "Train: metrics  0.8235108303249098 loss  0.3851020627369304\n",
      "Expected LB value: 0.5409839594421039 0.552655372821812\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b39ebb6d754293ae0ea670c3ed3933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2770.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04937fd3c6d0486086d40bc1224a8812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1213.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: metrics  0.636911145945054 loss  1.0393574458377257\n",
      "Train: metrics  0.8593185920577617 loss  0.3182312691537159\n",
      "Expected LB value: 1.1216374458377256 1.2522378865514767\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbed2b12d46a4b2fa9284ee0c84cc8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2770.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406dfaef3c4d48609e1113f53b3a01d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1213.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: metrics  0.7947364078871381 loss  0.48183109777313404\n",
      "Train: metrics  0.8795803249097472 loss  0.2786013092159795\n",
      "Expected LB value: 0.5641110977731341 0.5805193949073905\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8355ecda107545d48b5f79a5ff52b7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2770.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-a375b780ae36>\", line 13, in <module>\n",
      "    reverse=False\n",
      "  File \"C:\\Users\\user\\Desktop\\Projects\\dfdc-kaggle-solution\\pipeline\\model_methods.py\", line 183, in train_img\n",
      "    loss_value.backward()\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"d:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#%%script false\n",
    "# 38% / 47% / 57% /58.5% / 61% - 0.656% (0.99344%) - video\n",
    "# 83% undersampling / 50% oversampling / 89.8% original dataset / 0.08228\n",
    "\n",
    "metric, lost = train_img(model, loss, optimizer, scheduler, x_train, x_val, y_train, accuracy_b, device, [log_loss_sigmoid, accuracy_sigmoid_mean], None, \n",
    "                          epochs=10, \n",
    "                       batch_size=batch_size, \n",
    "                       del_net=False, \n",
    "                       useInference=False, \n",
    "                       inference=False, \n",
    "                       useScheduler=False,\n",
    "                       checkpoint=[0.37, 0.99],\n",
    "                         reverse=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "min_lost = max(enumerate(lost), key=itemgetter(1))[0] \n",
    "min_metrics = min(enumerate(metric), key=itemgetter(1))[0] \n",
    "print(\"Loss optim:\", lost[min_lost], min_lost + 1)\n",
    "print(\"Metrics optim:\", metric[min_metrics], min_metrics + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(metric, label='model')\n",
    "plt.legend()\n",
    "plt.title('Validation metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lost, label='model')\n",
    "plt.legend()\n",
    "plt.title('Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "# default - Validation: metrics  0.6033333333333334 loss:  0.6769222617149353\n",
    "\n",
    "validate_vid_bf(model, x_val_vid, y_train, log_loss_sigmoid, accuracy_sigmoid_mean, device, 2, face_extractor, \n",
    "                fast_mtcnn=fast_mtcnn,\n",
    "                print_results=True, \n",
    "                reverse=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
