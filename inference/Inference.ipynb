{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randint\n",
    "from PIL import ImageFilter, Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "sys.path.append('../input/efficientnet')\n",
    "sys.path.append('../input/imutils/imutils-0.5.3')\n",
    "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
    "sys.path.insert(0, \"/kaggle/input/helpers\")\n",
    "sys.path.insert(0, \"/kaggle/input/timmmodels\") \n",
    "\n",
    "import timm\n",
    "from imutils.video import FileVideoStream \n",
    "from efficientnet import EfficientNet\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
    "\n",
    "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
    "len(test_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazeface import BlazeFace\n",
    "facedet = BlazeFace().to(device)\n",
    "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n",
    "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n",
    "_ = facedet.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_grad(model):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "    return model\n",
    "        \n",
    "\n",
    "def weight_preds(preds, weights):\n",
    "    final_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(len(preds[i])):\n",
    "            if len(final_preds) != len(preds[i]):\n",
    "                final_preds.append(preds[i][j] * weights[i])\n",
    "            else:\n",
    "                final_preds[j] += preds[i][j] * weights[i]\n",
    "                \n",
    "    return torch.FloatTensor(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.read_video_1 import VideoReader\n",
    "from helpers.face_extract_1 import FaceExtractor\n",
    "\n",
    "frames_per_video = 120\n",
    "\n",
    "video_reader = VideoReader()\n",
    "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
    "face_extractor = FaceExtractor(video_read_fn, facedet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, models=None, device='cuda:0', extended=False):\n",
    "        super(MetaModel, self).__init__()\n",
    "        \n",
    "        self.extended = extended\n",
    "        self.device = device\n",
    "        self.models = models\n",
    "        self.len = len(models)\n",
    "        \n",
    "        if self.extended:\n",
    "            self.bn = nn.BatchNorm1d(self.len)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc = nn.Linear(self.len, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.cat(tuple(x), dim=1)\n",
    "        \n",
    "        if self.extended:\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            #x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = \"/kaggle/input/deepfake-detection-model-20k/\"\n",
    "WEIGTHS_EXT = '.pth'\n",
    "\n",
    "models = []\n",
    "weigths = []\n",
    "    \n",
    "raw_data_stack = \\\n",
    "[\n",
    "    ['0.8548137313946486 0.3376769562025044', 'efficientnet-b2'],\n",
    "    ['EfficientNetb3 0.8573518024606384 0.34558522378585194', 'efficientnet-b3'],\n",
    "    ['EfficientNetb4 0.8579110384582294 0.3383911053075265', 'efficientnet-b4'],\n",
    "    ['EfficientNet6 0.8602770369095758 0.33193617861157143', 'efficientnet-b6'],\n",
    "    ['EfficientNetb0 t2 0.8616966359803837 0.3698434531609828', 'efficientnet-b0'],\n",
    "    ['EfficientNetb1 t2 0.8410909403768391 0.36058002083572327', 'efficientnet-b1'],\n",
    "    ['EfficientNetb2 t2 0.8659554331928073 0.35598630783834084', 'efficientnet-b2'],\n",
    "    ['EfficientNetb3 t2 0.8486191172674868 0.3611779548592305', 'efficientnet-b3'],\n",
    "    ['EfficientNetb3 0.8635894347414609 0.328333642473084', 'efficientnet-b3'],\n",
    "    ['EfficientNetb6 0.8593736556826981 0.32286693639934694', 'efficientnet-b6'],\n",
    "    \n",
    "    ['tf_efficientnet_b1_ns 0.8571367116923342 0.3341234226295108', 'tf_efficientnet_b1_ns'],\n",
    "    ['tf_efficientnet_b3_ns 0.8712466660930913 0.3277394129117183', 'tf_efficientnet_b3_ns'],\n",
    "    ['tf_efficientnet_b4_ns 0.8708595027101437 0.3152573955405342', 'tf_efficientnet_b4_ns'],\n",
    "    ['tf_efficientnet_b6_ns 0.8733115374688118 0.3156576980666498', 'tf_efficientnet_b6_ns'],\n",
    "]\n",
    "\n",
    "stack_models = []\n",
    "\n",
    "for raw_model in raw_data_stack:\n",
    "    checkpoint = torch.load( MODELS_PATH + raw_model[0] + WEIGTHS_EXT, map_location=device)\n",
    "    \n",
    "    if '-' in raw_model[1]:\n",
    "        model = EfficientNet.from_name(raw_model[1])\n",
    "        model._fc = nn.Linear(model._fc.in_features, 1)\n",
    "    else:\n",
    "        model = timm.create_model(raw_model[1], pretrained=False)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    _ = model.eval()\n",
    "    _ = disable_grad(model)\n",
    "    model = model.to(device)\n",
    "    stack_models.append(model)\n",
    "\n",
    "    del checkpoint, model\n",
    "    \n",
    "\n",
    "meta_models = \\\n",
    "[\n",
    "    ['MetaModel 0.30638167556896007', slice(4, 8), False, 0.37780],\n",
    "    ['MetaModel 0.2919331893755284', slice(0, 4), False, 0.33357],\n",
    "    ['MetaModel 0.30281482560578044', slice(0, 8, None), True, 0.34077],\n",
    "    ['MetaModel 0.26302117601197256', slice(0, 10, None), False, 0.35134],\n",
    "    ['MetaModel 0.256337642808031', slice(10, 14, None), False, 0.32698],\n",
    "]\n",
    "\n",
    "for meta_raw in meta_models:\n",
    "\n",
    "    checkpoint = torch.load(MODELS_PATH + meta_raw[0] + WEIGTHS_EXT, map_location=device)\n",
    "    \n",
    "    model = MetaModel(models=raw_data_stack[meta_raw[1]], extended=meta_raw[2]).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    _ = model.eval()\n",
    "    _ = disable_grad(model)\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "    weigths.append(meta_raw[3])\n",
    "\n",
    "    del model, checkpoint\n",
    "    \n",
    "total = sum([1-score for score in weigths])\n",
    "weigths = [(1-score) / total for score in weigths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_video(video_path, batch_size):\n",
    "    try:\n",
    "        # Find the faces for N frames in the video.\n",
    "        faces = face_extractor.process_video(video_path)\n",
    "\n",
    "        # Only look at one face per frame.\n",
    "        face_extractor.keep_only_best_face(faces)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # NOTE: When running on the CPU, the batch size must be fixed\n",
    "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
    "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "\n",
    "            # If we found any faces, prepare them for the model.\n",
    "            n = 0\n",
    "            for frame_data in faces:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    # Resize to the model's required input size.\n",
    "                    resized_face = cv2.resize(face, (input_size, input_size))\n",
    "                    \n",
    "                    if n < batch_size:\n",
    "                        x[n] = resized_face\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
    "\n",
    "                    # Test time augmentation: horizontal flips.\n",
    "                    # TODO: not sure yet if this helps or not\n",
    "                    #x[n] = cv2.flip(resized_face, 1)\n",
    "                    #n += 1\n",
    "\n",
    "            del faces\n",
    "\n",
    "            if n > 0:\n",
    "                x = torch.tensor(x, device=device).float()\n",
    "\n",
    "                # Preprocess the images.\n",
    "                x = x.permute((0, 3, 1, 2))\n",
    "\n",
    "                for i in range(len(x)):\n",
    "                    x[i] = normalize_transform(x[i] / 255.)\n",
    "\n",
    "                # Make a prediction\n",
    "                with torch.no_grad():\n",
    "                    y_pred = 0\n",
    "                    stacked_preds = []\n",
    "                    preds = []\n",
    "                    \n",
    "                    for i in range(len(stack_models)):\n",
    "                        stacked_preds.append(stack_models[i](x).squeeze()[:n].unsqueeze(dim=1))\n",
    "                    \n",
    "                    for i in range(len(models)):\n",
    "                        preds.append(models[i](stacked_preds[meta_models[i][1]]))\n",
    "                \n",
    "                    del x, stacked_preds\n",
    "                    \n",
    "                    y_pred = torch.sigmoid(weight_preds(preds, weigths)).mean().item()\n",
    "                    \n",
    "                    del preds\n",
    "                    \n",
    "                    return y_pred\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
    "    \n",
    "    \n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "\n",
    "def predict_on_video_set(videos, num_workers):\n",
    "    def process_file(i):\n",
    "        filename = videos[i]\n",
    "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
    "        predictions = ex.map(process_file, range(len(videos)))\n",
    "        \n",
    "    return list(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_on_video_set(test_videos, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
